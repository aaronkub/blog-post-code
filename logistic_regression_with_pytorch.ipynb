{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barebones Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/IRIS.csv\").drop(\"Id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {val: index for index, val in enumerate(data.Species.unique())}\n",
    "X_numpy = data.drop(\"Species\", axis=1).values\n",
    "y_numpy = data.Species.map(target_map).values\n",
    "\n",
    "X = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "y = torch.tensor(y_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encode Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    # assumes that vector is one dimentional\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes)).type(torch.LongTensor)\n",
    "    return one_hot.scatter(1, vector.type(torch.LongTensor).unsqueeze(1), 1)\n",
    "\n",
    "y_one_hot = one_hot_encode(y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x124edcb10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((4, 3))\n",
    "b = torch.rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Softmax Activation and Cross Entropy Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(z: Tensor) -> Tensor:\n",
    "    exponentials: Tensor = torch.exp(z)\n",
    "    exponentials_row_sums: Tensor = torch.sum(exponentials, axis=1).unsqueeze(1)\n",
    "    return exponentials / exponentials_row_sums\n",
    "\n",
    "def cross_entropy_loss(targets: Tensor, activations: Tensor) -> Tensor:\n",
    "    return torch.mean(-torch.log(torch.sum(targets * activations, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 0.6981450319290161\n",
      "Loss at iteration 20: 0.6961764693260193\n",
      "Loss at iteration 30: 0.6425224542617798\n",
      "Loss at iteration 40: 0.602511465549469\n",
      "Loss at iteration 50: 0.5691211223602295\n",
      "Loss at iteration 60: 0.5393685698509216\n",
      "Loss at iteration 70: 0.5117704272270203\n",
      "Loss at iteration 80: 0.48551255464553833\n",
      "Loss at iteration 90: 0.4601267874240875\n",
      "Loss at iteration 100: 0.4353489577770233\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X, w) + b\n",
    "    predictions = softmax_activation(Z)\n",
    "    loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "    w_gradients = -torch.mm(X.transpose(0, 1), y_one_hot - predictions) / X.shape[0]\n",
    "    b_gradients = -torch.mean(y_one_hot - predictions, axis=0)\n",
    "    \n",
    "    w -= learning_rate * w_gradients\n",
    "    b -= learning_rate * b_gradients\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Differentiation with PyTorch's Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_autograd = torch.rand((4, 3), requires_grad=True)\n",
    "b_autograd = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.mm(X, w_autograd) + b_autograd\n",
    "predictions = softmax_activation(Z)\n",
    "loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5716,  1.4152, -0.8436],\n",
       "        [-0.5902,  0.8152, -0.2250],\n",
       "        [ 0.2740,  0.8346, -1.1086],\n",
       "        [ 0.1701,  0.2812, -0.4513]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5716,  1.4152, -0.8436],\n",
       "        [-0.5902,  0.8152, -0.2250],\n",
       "        [ 0.2740,  0.8346, -1.1086],\n",
       "        [ 0.1701,  0.2812, -0.4513]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.mm(X.transpose(0, 1), y_one_hot - predictions.detach()) / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 0.954537034034729\n",
      "Loss at iteration 20: 0.7833609580993652\n",
      "Loss at iteration 30: 0.7053467035293579\n",
      "Loss at iteration 40: 0.6537990570068359\n",
      "Loss at iteration 50: 0.6124182343482971\n",
      "Loss at iteration 60: 0.5758307576179504\n",
      "Loss at iteration 70: 0.5418170690536499\n",
      "Loss at iteration 80: 0.5093513131141663\n",
      "Loss at iteration 90: 0.47795018553733826\n",
      "Loss at iteration 100: 0.4474251866340637\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    \n",
    "    \n",
    "    Z = torch.mm(X, w_autograd) + b_autograd\n",
    "    predictions = softmax_activation(Z)\n",
    "    loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fight Overfitting with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regularized = torch.rand((4, 3), requires_grad=True)\n",
    "b_regularized = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0.5\n",
    "\n",
    "Z = torch.mm(X, w_regularized) + b_regularized\n",
    "predictions = softmax_activation(Z)\n",
    "loss = cross_entropy_loss(y_one_hot, predictions) - l * torch.sum(w_regularized ** 2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7040,  2.4630, -2.3124],\n",
       "        [-1.2814,  1.1602, -1.3662],\n",
       "        [-1.2611,  0.9958, -1.9769],\n",
       "        [-0.5951, -0.2039, -1.1867]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_regularized.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7040,  2.4630, -2.3124],\n",
       "        [-1.2814,  1.1602, -1.3662],\n",
       "        [-1.2611,  0.9958, -1.9769],\n",
       "        [-0.5951, -0.2039, -1.1867]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.mm(X.transpose(0, 1), y_one_hot - predictions) / X.shape[0] - (2 * l * w_regularized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 1.0347145795822144\n",
      "Loss at iteration 20: 0.8628233671188354\n",
      "Loss at iteration 30: 0.779863178730011\n",
      "Loss at iteration 40: 0.7254924178123474\n",
      "Loss at iteration 50: 0.6824817657470703\n",
      "Loss at iteration 60: 0.6448083519935608\n",
      "Loss at iteration 70: 0.6099228858947754\n",
      "Loss at iteration 80: 0.5766125321388245\n",
      "Loss at iteration 90: 0.5442736148834229\n",
      "Loss at iteration 100: 0.5126192569732666\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "for i in range(1, n_iterations + 1):\n",
    "    Z = model(X)\n",
    "    loss = cross_entropy_loss(Z, y)\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    sgd_optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8972, 0.1016, 0.0012],\n",
       "        [0.8358, 0.1614, 0.0028],\n",
       "        [0.8750, 0.1228, 0.0022],\n",
       "        [0.8312, 0.1648, 0.0040],\n",
       "        [0.9060, 0.0928, 0.0012],\n",
       "        [0.8905, 0.1082, 0.0013],\n",
       "        [0.8759, 0.1215, 0.0026],\n",
       "        [0.8739, 0.1242, 0.0019],\n",
       "        [0.8158, 0.1790, 0.0052],\n",
       "        [0.8442, 0.1532, 0.0026]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(model(X), 1)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
