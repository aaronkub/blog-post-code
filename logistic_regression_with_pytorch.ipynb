{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barebones Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/IRIS.csv\").drop(\"Id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {val: index for index, val in enumerate(data.Species.unique())}\n",
    "X_numpy = data.drop(\"Species\", axis=1).values\n",
    "y_numpy = data.Species.map(target_map).values\n",
    "\n",
    "X = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "y = torch.tensor(y_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encode Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vector, n_classes):\n",
    "    # assumes that vector is one dimentional\n",
    "    one_hot = torch.zeros((vector.shape[0], n_classes)).type(torch.LongTensor)\n",
    "    return one_hot.scatter(1, vector.type(torch.LongTensor).unsqueeze(1), 1)\n",
    "\n",
    "y_one_hot = one_hot_encode(y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12554d930>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((4, 3))\n",
    "b = torch.rand(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Softmax Activation and Cross Entropy Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_activation(z: Tensor) -> Tensor:\n",
    "    exponentials: Tensor = torch.exp(z)\n",
    "    exponentials_row_sums: Tensor = torch.sum(exponentials, axis=1).unsqueeze(1)\n",
    "    return exponentials / exponentials_row_sums\n",
    "\n",
    "def cross_entropy_loss(targets: Tensor, activations: Tensor) -> Tensor:\n",
    "    return torch.mean(-torch.log(torch.sum(targets * activations, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 0.6981450319290161\n",
      "Loss at iteration 20: 0.6961764693260193\n",
      "Loss at iteration 30: 0.6425224542617798\n",
      "Loss at iteration 40: 0.602511465549469\n",
      "Loss at iteration 50: 0.5691211223602295\n",
      "Loss at iteration 60: 0.5393685698509216\n",
      "Loss at iteration 70: 0.5117704272270203\n",
      "Loss at iteration 80: 0.48551255464553833\n",
      "Loss at iteration 90: 0.4601267874240875\n",
      "Loss at iteration 100: 0.4353489577770233\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    \n",
    "    Z = torch.mm(X, w) + b\n",
    "    predictions = softmax_activation(Z)\n",
    "    loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "    w_gradients = -torch.mm(X.transpose(0, 1), y_one_hot - predictions) / X.shape[0]\n",
    "    b_gradients = -torch.mean(y_one_hot - predictions, axis=0)\n",
    "    \n",
    "    w -= learning_rate * w_gradients\n",
    "    b -= learning_rate * b_gradients\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Differentiation with PyTorch's Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_autograd = torch.rand((4, 3), requires_grad=True)\n",
    "b_autograd = torch.rand(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.mm(X, w_autograd) + b_autograd\n",
    "predictions = softmax_activation(Z)\n",
    "loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5716,  1.4152, -0.8436],\n",
      "        [-0.5902,  0.8152, -0.2250],\n",
      "        [ 0.2740,  0.8346, -1.1086],\n",
      "        [ 0.1701,  0.2812, -0.4513]])\n"
     ]
    }
   ],
   "source": [
    "print(w_autograd.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5716,  1.4152, -0.8436],\n",
      "        [-0.5902,  0.8152, -0.2250],\n",
      "        [ 0.2740,  0.8346, -1.1086],\n",
      "        [ 0.1701,  0.2812, -0.4513]])\n"
     ]
    }
   ],
   "source": [
    "print(-torch.mm(X.transpose(0, 1), y_one_hot - predictions.detach()) / X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 0.954537034034729\n",
      "Loss at iteration 20: 0.7833609580993652\n",
      "Loss at iteration 30: 0.7053467035293579\n",
      "Loss at iteration 40: 0.6537990570068359\n",
      "Loss at iteration 50: 0.6124182343482971\n",
      "Loss at iteration 60: 0.5758307576179504\n",
      "Loss at iteration 70: 0.5418170690536499\n",
      "Loss at iteration 80: 0.5093513131141663\n",
      "Loss at iteration 90: 0.47795018553733826\n",
      "Loss at iteration 100: 0.4474251866340637\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "learning_rate = 0.1\n",
    "for i in range(1, n_iterations + 1):\n",
    "    if w_autograd.grad is not None:\n",
    "        w_autograd.grad.zero_()\n",
    "    if b_autograd.grad is not None:\n",
    "        b_autograd.grad.zero_()\n",
    "    \n",
    "    \n",
    "    Z = torch.mm(X, w_autograd) + b_autograd\n",
    "    predictions = softmax_activation(Z)\n",
    "    loss = cross_entropy_loss(y_one_hot, predictions)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_autograd -= learning_rate * w_autograd.grad\n",
    "        b_autograd -= learning_rate * b_autograd.grad\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10: 0.8743402361869812\n",
      "Loss at iteration 20: 0.7898827791213989\n",
      "Loss at iteration 30: 0.7347971200942993\n",
      "Loss at iteration 40: 0.6914253830909729\n",
      "Loss at iteration 50: 0.6535581946372986\n",
      "Loss at iteration 60: 0.6185600161552429\n",
      "Loss at iteration 70: 0.5851718783378601\n",
      "Loss at iteration 80: 0.5527639985084534\n",
      "Loss at iteration 90: 0.5210314989089966\n",
      "Loss at iteration 100: 0.48985880613327026\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 100\n",
    "for i in range(1, n_iterations + 1):\n",
    "    Z = model(X)\n",
    "    loss = cross_entropy_loss(Z, y)\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    sgd_optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Loss at iteration {}: {}\".format(i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.0356e-01, 9.5569e-02, 8.7547e-04],\n",
       "        [8.4530e-01, 1.5261e-01, 2.0888e-03],\n",
       "        [8.8109e-01, 1.1729e-01, 1.6244e-03],\n",
       "        [8.3592e-01, 1.6093e-01, 3.1513e-03],\n",
       "        [9.1098e-01, 8.8168e-02, 8.5557e-04],\n",
       "        [8.9638e-01, 1.0269e-01, 9.2407e-04],\n",
       "        [8.7987e-01, 1.1816e-01, 1.9702e-03],\n",
       "        [8.8006e-01, 1.1855e-01, 1.3846e-03],\n",
       "        [8.2086e-01, 1.7505e-01, 4.0849e-03],\n",
       "        [8.5133e-01, 1.4667e-01, 2.0017e-03],\n",
       "        [9.1331e-01, 8.6087e-02, 5.9965e-04],\n",
       "        [8.6247e-01, 1.3540e-01, 2.1356e-03],\n",
       "        [8.5283e-01, 1.4512e-01, 2.0530e-03],\n",
       "        [8.8803e-01, 1.1010e-01, 1.8690e-03],\n",
       "        [9.5678e-01, 4.3091e-02, 1.3126e-04],\n",
       "        [9.5093e-01, 4.8858e-02, 2.1680e-04],\n",
       "        [9.3519e-01, 6.4429e-02, 3.8330e-04],\n",
       "        [8.9702e-01, 1.0198e-01, 9.9740e-04],\n",
       "        [8.9747e-01, 1.0187e-01, 6.6552e-04],\n",
       "        [9.1237e-01, 8.6801e-02, 8.3276e-04],\n",
       "        [8.5724e-01, 1.4139e-01, 1.3694e-03],\n",
       "        [8.9733e-01, 1.0159e-01, 1.0840e-03],\n",
       "        [9.4172e-01, 5.7726e-02, 5.4924e-04],\n",
       "        [8.0670e-01, 1.9016e-01, 3.1381e-03],\n",
       "        [8.0982e-01, 1.8617e-01, 4.0049e-03],\n",
       "        [8.1082e-01, 1.8634e-01, 2.8424e-03],\n",
       "        [8.4794e-01, 1.4985e-01, 2.2124e-03],\n",
       "        [8.9313e-01, 1.0589e-01, 9.7483e-04],\n",
       "        [8.9558e-01, 1.0352e-01, 8.9522e-04],\n",
       "        [8.3430e-01, 1.6263e-01, 3.0720e-03],\n",
       "        [8.2181e-01, 1.7507e-01, 3.1218e-03],\n",
       "        [8.7025e-01, 1.2859e-01, 1.1543e-03],\n",
       "        [9.4312e-01, 5.6499e-02, 3.8114e-04],\n",
       "        [9.5284e-01, 4.6940e-02, 2.1776e-04],\n",
       "        [8.5133e-01, 1.4667e-01, 2.0017e-03],\n",
       "        [8.9783e-01, 1.0123e-01, 9.4074e-04],\n",
       "        [9.1833e-01, 8.1214e-02, 4.5236e-04],\n",
       "        [8.5133e-01, 1.4667e-01, 2.0017e-03],\n",
       "        [8.5248e-01, 1.4460e-01, 2.9208e-03],\n",
       "        [8.8154e-01, 1.1722e-01, 1.2406e-03],\n",
       "        [9.0711e-01, 9.1997e-02, 8.9535e-04],\n",
       "        [7.2652e-01, 2.6655e-01, 6.9358e-03],\n",
       "        [8.7645e-01, 1.2129e-01, 2.2576e-03],\n",
       "        [8.4218e-01, 1.5531e-01, 2.5090e-03],\n",
       "        [8.5275e-01, 1.4501e-01, 2.2466e-03],\n",
       "        [8.3351e-01, 1.6385e-01, 2.6423e-03],\n",
       "        [9.0784e-01, 9.1252e-02, 9.0934e-04],\n",
       "        [8.6525e-01, 1.3250e-01, 2.2458e-03],\n",
       "        [9.1222e-01, 8.7106e-02, 6.6957e-04],\n",
       "        [8.8279e-01, 1.1594e-01, 1.2719e-03],\n",
       "        [4.8562e-02, 8.6011e-01, 9.1328e-02],\n",
       "        [5.0987e-02, 8.1357e-01, 1.3544e-01],\n",
       "        [3.0353e-02, 8.3570e-01, 1.3395e-01],\n",
       "        [3.5961e-02, 7.3852e-01, 2.2552e-01],\n",
       "        [3.0152e-02, 8.1021e-01, 1.5964e-01],\n",
       "        [3.1737e-02, 7.2107e-01, 2.4719e-01],\n",
       "        [3.8575e-02, 7.8066e-01, 1.8077e-01],\n",
       "        [1.1076e-01, 7.3440e-01, 1.5485e-01],\n",
       "        [4.0230e-02, 8.3432e-01, 1.2545e-01],\n",
       "        [5.2731e-02, 7.0933e-01, 2.3794e-01],\n",
       "        [5.8175e-02, 7.3767e-01, 2.0416e-01],\n",
       "        [5.4529e-02, 7.7702e-01, 1.6845e-01],\n",
       "        [4.7842e-02, 8.2103e-01, 1.3113e-01],\n",
       "        [2.7874e-02, 7.5280e-01, 2.1933e-01],\n",
       "        [1.1671e-01, 7.7269e-01, 1.1060e-01],\n",
       "        [6.0832e-02, 8.4687e-01, 9.2302e-02],\n",
       "        [3.1934e-02, 6.9278e-01, 2.7528e-01],\n",
       "        [6.6055e-02, 7.9431e-01, 1.3964e-01],\n",
       "        [1.6633e-02, 7.5343e-01, 2.2994e-01],\n",
       "        [6.1919e-02, 7.8063e-01, 1.5745e-01],\n",
       "        [2.1760e-02, 6.7361e-01, 3.0464e-01],\n",
       "        [7.1439e-02, 8.1610e-01, 1.1246e-01],\n",
       "        [1.3145e-02, 7.2034e-01, 2.6652e-01],\n",
       "        [2.9609e-02, 7.6634e-01, 2.0405e-01],\n",
       "        [5.7191e-02, 8.3122e-01, 1.1159e-01],\n",
       "        [5.3851e-02, 8.4080e-01, 1.0535e-01],\n",
       "        [2.6848e-02, 8.3254e-01, 1.4061e-01],\n",
       "        [1.9016e-02, 7.8038e-01, 2.0060e-01],\n",
       "        [3.3146e-02, 7.5587e-01, 2.1098e-01],\n",
       "        [1.2543e-01, 7.8886e-01, 8.5706e-02],\n",
       "        [6.2286e-02, 7.7556e-01, 1.6216e-01],\n",
       "        [7.6906e-02, 7.8455e-01, 1.3854e-01],\n",
       "        [7.3920e-02, 7.9632e-01, 1.2975e-01],\n",
       "        [9.5987e-03, 6.1838e-01, 3.7202e-01],\n",
       "        [2.9379e-02, 6.5410e-01, 3.1652e-01],\n",
       "        [5.1769e-02, 7.6292e-01, 1.8531e-01],\n",
       "        [3.7894e-02, 8.3032e-01, 1.3178e-01],\n",
       "        [2.6142e-02, 8.0484e-01, 1.6902e-01],\n",
       "        [6.7136e-02, 7.5814e-01, 1.7472e-01],\n",
       "        [4.4504e-02, 7.4568e-01, 2.0981e-01],\n",
       "        [2.9963e-02, 7.0158e-01, 2.6845e-01],\n",
       "        [3.5843e-02, 7.6997e-01, 1.9418e-01],\n",
       "        [5.8503e-02, 7.9234e-01, 1.4915e-01],\n",
       "        [1.0321e-01, 7.4792e-01, 1.4887e-01],\n",
       "        [4.2682e-02, 7.4271e-01, 2.1461e-01],\n",
       "        [6.5223e-02, 7.6815e-01, 1.6663e-01],\n",
       "        [5.4361e-02, 7.6183e-01, 1.8381e-01],\n",
       "        [5.4483e-02, 8.1265e-01, 1.3286e-01],\n",
       "        [1.7121e-01, 7.3338e-01, 9.5417e-02],\n",
       "        [5.6308e-02, 7.6933e-01, 1.7436e-01],\n",
       "        [1.8960e-03, 3.8361e-01, 6.1449e-01],\n",
       "        [6.3980e-03, 5.2526e-01, 4.6835e-01],\n",
       "        [3.8816e-03, 6.3390e-01, 3.6222e-01],\n",
       "        [5.0433e-03, 5.5624e-01, 4.3872e-01],\n",
       "        [2.9935e-03, 5.0012e-01, 4.9689e-01],\n",
       "        [1.5572e-03, 5.8040e-01, 4.1805e-01],\n",
       "        [1.0288e-02, 4.6960e-01, 5.2012e-01],\n",
       "        [2.6298e-03, 6.2045e-01, 3.7692e-01],\n",
       "        [2.7766e-03, 5.6329e-01, 4.3393e-01],\n",
       "        [4.0125e-03, 6.0458e-01, 3.9141e-01],\n",
       "        [1.4638e-02, 7.0924e-01, 2.7612e-01],\n",
       "        [6.3124e-03, 6.1828e-01, 3.7541e-01],\n",
       "        [6.4543e-03, 6.5898e-01, 3.3457e-01],\n",
       "        [5.0845e-03, 4.9057e-01, 5.0434e-01],\n",
       "        [4.2792e-03, 4.5520e-01, 5.4052e-01],\n",
       "        [7.6841e-03, 6.0395e-01, 3.8837e-01],\n",
       "        [7.4401e-03, 6.3601e-01, 3.5655e-01],\n",
       "        [3.2172e-03, 6.4014e-01, 3.5665e-01],\n",
       "        [4.8694e-04, 4.5533e-01, 5.4418e-01],\n",
       "        [6.9679e-03, 6.1170e-01, 3.8134e-01],\n",
       "        [5.0942e-03, 6.2403e-01, 3.7087e-01],\n",
       "        [8.2083e-03, 5.2066e-01, 4.7113e-01],\n",
       "        [1.2077e-03, 5.7542e-01, 4.2337e-01],\n",
       "        [1.2571e-02, 6.9787e-01, 2.8956e-01],\n",
       "        [6.2815e-03, 6.1742e-01, 3.7630e-01],\n",
       "        [5.7612e-03, 6.9296e-01, 3.0128e-01],\n",
       "        [1.5757e-02, 7.0487e-01, 2.7937e-01],\n",
       "        [1.6194e-02, 6.7992e-01, 3.0389e-01],\n",
       "        [3.4929e-03, 5.2278e-01, 4.7373e-01],\n",
       "        [7.5039e-03, 7.4258e-01, 2.4992e-01],\n",
       "        [3.0969e-03, 6.6555e-01, 3.3135e-01],\n",
       "        [6.6965e-03, 7.6746e-01, 2.2584e-01],\n",
       "        [3.1495e-03, 5.0666e-01, 4.9019e-01],\n",
       "        [1.3507e-02, 7.0339e-01, 2.8311e-01],\n",
       "        [4.7112e-03, 5.4219e-01, 4.5310e-01],\n",
       "        [3.0682e-03, 6.9061e-01, 3.0633e-01],\n",
       "        [4.9849e-03, 5.0969e-01, 4.8532e-01],\n",
       "        [7.9571e-03, 6.2241e-01, 3.6963e-01],\n",
       "        [1.8145e-02, 6.7960e-01, 3.0225e-01],\n",
       "        [8.8426e-03, 7.0887e-01, 2.8229e-01],\n",
       "        [4.3791e-03, 5.7688e-01, 4.1874e-01],\n",
       "        [1.1724e-02, 7.4148e-01, 2.4679e-01],\n",
       "        [6.3980e-03, 5.2526e-01, 4.6835e-01],\n",
       "        [3.4470e-03, 5.5162e-01, 4.4493e-01],\n",
       "        [4.2359e-03, 5.5569e-01, 4.4008e-01],\n",
       "        [8.2777e-03, 6.7544e-01, 3.1628e-01],\n",
       "        [7.7924e-03, 6.4716e-01, 3.4505e-01],\n",
       "        [9.9962e-03, 6.7414e-01, 3.1587e-01],\n",
       "        [7.3782e-03, 5.5138e-01, 4.4124e-01],\n",
       "        [1.0666e-02, 5.9263e-01, 3.9670e-01]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(model(X), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
